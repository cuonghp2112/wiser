{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to WISER, Part 1: Tagging and Linking Rules\n",
    "\n",
    "Welcome to WISER (*Weak and Indirect Supervision for Entity Recognition*), a system for training sequence-to-sequence models, particularly neural networks for named entity recognition (NER) and related tasks. WISER uses *weak supervision* in the form of rules to train these models, as opposed to hand-labeled training data.\n",
    "\n",
    "In this first part of the tutorial, we will be writing tagging and linking rules to identify award names, media performances from a Wikipedia text corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "WISER is an add-on to [AllenNLP](http://allennlp.org), a great framework for natural language processing. That means we can use their tools for working with data.\n",
    "\n",
    "Let's start by loading the Media dataset, a new dataset we created just for this tutorial. \n",
    "import os\n",
    "os.chdir(/Users/yueyang/Documents/bats/wiser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1823it [00:00, 18227.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2617it [00:00, 13123.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4366it [00:00, 12270.68it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6154it [00:00, 13544.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7219it [00:00, 10756.08it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7422it [00:00, 11918.06it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "653it [00:00, 17201.97it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "750it [00:00, 16056.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from wiser.data.dataset_readers import MediaDatasetReader\n",
    "print(os.getcwd())\n",
    "dataset_reader = MediaDatasetReader()\n",
    "train_data = dataset_reader.read('/Users/yueyang/Documents/bats/wiser/data/wikipedia/unlabeled_train.csv')\n",
    "dev_data = dataset_reader.read('/Users/yueyang/Documents/bats/wiser/data/wikipedia/labeled_dev.csv')\n",
    "test_data = dataset_reader.read('/Users/yueyang/Documents/bats/wiser/data/wikipedia/labeled_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In this tutorial we will use a only 10 instances of the training data\n",
    "train_data = train_data[:10]\n",
    "dev_data = dev_data[:10]\n",
    "test_data = train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We must merge all partitions to apply the rules\n",
    "data = train_data + dev_data + test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to use WISER with other data sets is to implement a new subclass of AllenNLP's [DatasetReader](https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.dataset_reader.html#allennlp.data.dataset_readers.dataset_reader.DatasetReader). We have some additional examples in the package `wiser.data.dataset_readers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Data\n",
    "Once the data is loaded, we can use a WISER class called `Viewer` to inspect the sentences and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "require.undef('viewer');\n",
       "\n",
       "// NOTE: all elements should be selected using this.$el.find to avoid collisions with other Viewers\n",
       "\n",
       "define('viewer', [\"@jupyter-widgets/base\"], function(widgets) {\n",
       "    var ViewerView = widgets.DOMWidgetView.extend({\n",
       "        // Main rendering function\n",
       "        render: function() {\n",
       "            // Insert the html\n",
       "            this.$el.append(this.model.get('html'));\n",
       "            this.nPages = this.model.get('n_instances');\n",
       "            this.$el.append(this.nPages);\n",
       "            // Set the instance id\n",
       "            this.id  = 0;\n",
       "            // Set the label source\n",
       "            this.source = 0;\n",
       "\n",
       "            // Enable buttons for changing page\n",
       "            var that = this;\n",
       "            this.$el.find(\"#next-page\").click(function() {\n",
       "                that.switchPage(1);\n",
       "            });\n",
       "            this.$el.find(\"#prev-page\").click(function() {\n",
       "                that.switchPage(-1);\n",
       "            });\n",
       "\n",
       "            // Enable select menu for changing label source\n",
       "            this.$el.find(\"#source\").change(function() {\n",
       "                that.switchSource();\n",
       "            })\n",
       "        },\n",
       "\n",
       "        switchPage: function(inc) {\n",
       "            this.$el.find(\".viewer-page\").hide();\n",
       "            if (this.id + inc < 0) {\n",
       "                this.id = 0;\n",
       "            } else if (this.id + inc >= this.nPages - 1) {\n",
       "                this.id = this.nPages - 1;\n",
       "            } else {\n",
       "                this.id += inc;\n",
       "            }\n",
       "            this.$el.find(\"#viewer-page-\"+this.id+\"-\"+this.source).show();\n",
       "\n",
       "            // Show page id\n",
       "            this.$el.find(\"#page\").html(this.id);\n",
       "        },\n",
       "\n",
       "        switchSource: function() {\n",
       "            this.source = this.$el.find(\"#source\").val();\n",
       "            this.switchPage(0);\n",
       "        }\n",
       "    });\n",
       "\n",
       "    return {\n",
       "        ViewerView: ViewerView\n",
       "    };\n",
       "});"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32a796b329745d4a7bed89461928395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Viewer(html='<head>\\n<style>\\nspan.active {\\n    background-color: skyblue;\\n    box-shadow: 1px 1px 1px grey;â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wiser.viewer import Viewer\n",
    "Viewer(dev_data, height=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the left and right buttons to flip through the items in `dev_data`, each of which is an AllenNLP [`Instance`](https://allenai.github.io/allennlp-docs/api/allennlp.data.instance.html#allennlp.data.instance.Instance). The highlighted spans are the entities, and you can hover over each one with your cursor to see whether it is an award (**AWD**), or a media performance **PERF**.\n",
    "\n",
    "The drop-down menu selects which source of labels is displayed. Currently only the gold labels from the benchmark are available, but we will add more soon.\n",
    "\n",
    "Advance to the instance at index 1 to see an example with multiple entities of different classes. You can access the underlying tags too by hovering over particular tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that WISER uses the [IOB1 tagging scheme](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)), meaning that entities are represented as consecutive tags beginning with **I**. Many data sets use subsequent characters for different classes, for example **-AWD** for awards and **-PERF** for movies, T.V. shows, or theatre plays. The **O**, or other tag, means that the token is not part of an entity. There is also a special set of tags beginning with **B** (like those beginning with **I**) that are used to start a new entity that immediately follows another of the same class without an **O** tag in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagging Rules\n",
    "Tagging rules are functions that map unlabeled text instances to sequences of labels. We can define our own tagging rules by writing small functions that look at sequences of instance tokens, and vote on their correponding tags. Let's first import the ``TaggingRule`` class from ``wiser.rules``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiser.rules import TaggingRule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Simple Tagging Rules\n",
    "From inspecting instance 11, we know tokens proper nouns followed by a year between parentheses are likely tagged as movies. For instance, the token ``Friends`` in the span ``Friends (1994 - 2004)`` should be tagged **I-PERF**. Let's  write our first tagging rule to reflect this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieYear(TaggingRule):\n",
    "    \n",
    "    def apply_instance(self, instance):\n",
    "\n",
    "        # Creates a list of tokens\n",
    "        tokens = [t.text for t in instance['tokens']]\n",
    "        \n",
    "        # Initializes a list of ABS (abstain) label votes \n",
    "        labels = ['ABS'] * len(tokens)\n",
    "        \n",
    "        for i in range(len(tokens)-2):    \n",
    "            # Tags proper nouns followed by a number between parentheses\n",
    "            if tokens[i].istitle() and tokens[i+1] == '(' and tokens[i+2].isdigit():\n",
    "                labels[i] = 'I-PERF'\n",
    "               \n",
    "        # Returns the modified label vote list\n",
    "        return labels\n",
    "\n",
    "# Applies the tagging rule to all dataset instances \n",
    "tr = MovieYear()\n",
    "tr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write a tagging rule to identify award categories like ``for Oustanding Lead Actress`` in award spans such as ``BAFTA Award for Oustanding Lead Actress``. Categories are generally preceded by capitalized letters, and follow with the strings ``for Oustanding`` or ``for Best``. Please refer instance at index 1 for an example of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AwardCategory(TaggingRule):\n",
    "    \n",
    "    def apply_instance(self, instance):\n",
    "\n",
    "        tokens = [t.text for t in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "        \n",
    "        for i in range(len(tokens)-2):\n",
    "            if tokens[i].istitle() and tokens[i+1] == 'for' and tokens[i+2] in {'Best', 'Oustanding'}:\n",
    "                labels[i+1] = 'I-AWD'\n",
    "                labels[i+2] = 'I-AWD'\n",
    "               \n",
    "        return labels\n",
    "\n",
    "tr = AwardCategory()\n",
    "tr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagging Function Helpers\n",
    "\n",
    "You can also use existing tagging functions and helpers available at `wiser.rules`. The ``DictionaryMatcher`` is a tagging function helper that allows us to quickly create a new rule that votes on any element found in a set of characters or words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiser.rules import DictionaryMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any token spelling ``Award`` , ``Awards``, ``Prize`` or ``Cup`` should be tagged as an award. Be mindful of capitalization, since awards are proper nouns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "award_keywords = [['Award'], ['Awards'], ['Prize'], ['Cup']]\n",
    "                  \n",
    "tr = DictionaryMatcher(\"AwardKeywords\", terms=award_keywords, i_label=\"I-AWD\", uncased=False)\n",
    "tr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good trick to developing efficient sequence taggers is to also generate negative supervision in the form of **O** tags. We can write the first function of this kind to tag some punctuations signs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_entity_punctuation_chars = {'.', ';', '(', ')'}\n",
    "\n",
    "tr = DictionaryMatcher(\"Non-EntityPunctuation\", terms=non_entity_punctuation_chars, i_label=\"O\")\n",
    "tr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend going over the data and identifying a few false positive tokens. That is, tokens that are similar to entities but are not (e.g., capitalized tokens such as studio names). We will also write a `DictionaryMatcher` identify some common false positives and tag them as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_false_positives = [['network'], ['netflix'], ['hulu'], ['bbc'], ['fox'], \n",
    "                          ['disney'], ['hbo'], ['CBS'], ['channel'], ['american'], \n",
    "                         ['showtime'], ['productions'], ['TV']]\n",
    "\n",
    "tr = DictionaryMatcher(\"CommonFalsePositives\", terms=common_false_positives, i_label=\"O\", uncased=True)\n",
    "tr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Previous Tagging Rules\n",
    "\n",
    "You can also develop more complex tagging rules by looking at previous tagging rule votes using the ``WISER_LABELS`` field. However, be mindful of the order in which you run the tagging functions.\n",
    "\n",
    "In the following example, we will write a tagging rule to identify performances based on adjacent tags such as ``series`` or ``show`` (e.g., ``The TV series The Mandalorian`` or ``Kung-Fu Panda franchise``). However, we also want to avoid tagging common false positives such as ``TV``), which is why we will reference the output votes of the ``CommonFalsePositives`` rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_keywords = {'trilogy', 'saga', 'series', 'miniseries', \n",
    "                'show', 'opera', 'drama', 'musical', 'sequel',\n",
    "                'prequel', 'franchise', 'thriller', 'sitcom'}\n",
    "\n",
    "class MovieKeywords(TaggingRule):\n",
    "    \n",
    "    def apply_instance(self, instance):\n",
    "\n",
    "        tokens = [t.text for t in instance['tokens']]\n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        # List of tag votes of CommonFalsePositives rule\n",
    "        false_positives = [t for t in instance['WISER_LABELS']['CommonFalsePositives']]\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            if tokens[i].lower() in movie_keywords:\n",
    "                \"\"\"\n",
    "                    We will only tag a word as a movie if \n",
    "                    the CommonFalsePositives asbtained from voting it.\n",
    "                    \n",
    "                    We also want to avoid award names \n",
    "                    like \"... Musical Drama\", etc.\n",
    "                \"\"\" \n",
    "                \n",
    "                # Keywords followed by movies (e.g., Kung-Fu Panda franchise)\n",
    "                if i < len(tokens) and tokens[i+1].istitle() and false_positives[i+1] == 'ABS':\n",
    "                    if tokens[i+1].lower() not in movie_keywords:\n",
    "                        labels[i+1] = 'I-PERF'\n",
    "                       \n",
    "                # Movies followed by keywords \n",
    "                elif i > 0 and tokens[i-1].istitle() and false_positives[i-1] == 'ABS':\n",
    "                    if tokens[i-1].lower() not in movie_keywords:\n",
    "                        labels[i-1] = 'I-PERF'\n",
    "        return labels\n",
    "\n",
    "tr = MovieKeywords()\n",
    "tr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Existing Models\n",
    "\n",
    "WISER also allows us to implement existing machine learning systems to provide more accurate weak supervision. You can import existing classifiers or NLP tools to improve your current model.\n",
    "\n",
    "For our next tagging rule, we will use [spaCy's part-of-speech tagger](https://spacy.io/usage/linguistic-features#pos-tagging). This pre-trained model identifies sentence part-of-speech tags, and will be useful to identify many non-entity words such as lowercased nouns, verbs, and adjectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "\n",
    "non_entity_lowercases = {'NOUN', 'VERB', 'ADJ', 'SPACE', 'NUM'}\n",
    "\n",
    "class NonEntityWords(TaggingRule):\n",
    "    \n",
    "    def apply_instance(self, instance):\n",
    "\n",
    "        tokens = [t.text for t in instance['tokens']]\n",
    "        \n",
    "        # We obtain the parts-of-speech from SpaCy\n",
    "        parts_of_speech = [token[0].pos_ for token in nlp.pipe(tokens)]        \n",
    "        labels = ['ABS'] * len(tokens)\n",
    "\n",
    "        for i, (token, pos) in enumerate(zip(tokens, parts_of_speech)):\n",
    "            if pos in non_entity_lowercases and not token.istitle():\n",
    "                labels[i] = 'O'\n",
    "                \n",
    "        return labels\n",
    "\n",
    "tr = NonEntityWords()\n",
    "tr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Tagging Rules\n",
    "We can inspect the performance of individual tagging_rules by using the ``score_tagging_rules`` method. \n",
    "\n",
    "* True positives (TP) represent the number of items correctly labeled spans belonging to a positive class (e.g. **I-PERF**).\n",
    "\n",
    "* False positives (FP) are the number of items incorrectly labeled spans belonging to a positive class.\n",
    "\n",
    "* False Negatives (FN) are the items which were not labeled as belonging to the positive class but should have been.\n",
    "\n",
    "* Token Accuracy (Token Acc.) represents the fraction of issued votes that correctly identified a token in a positive class.\n",
    "\n",
    "* Token Votes is the total number of times the tagging rules issued a vote belonging to a positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Token Acc.</th>\n",
       "      <th>Token Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AwardCategory</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AwardKeywords</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CommonFalsePositives</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MovieKeywords</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MovieYear</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Non-EntityPunctuation</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NonEntityWords</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0.9351</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       TP  FP  FN  Token Acc.  Token Votes\n",
       "AwardCategory           0   5  37      1.0000           10\n",
       "AwardKeywords           0  12  37      1.0000           12\n",
       "CommonFalsePositives    0   0  37      0.6667            3\n",
       "MovieKeywords           0   2  37      0.5000            2\n",
       "MovieYear               0   0  37         NaN            0\n",
       "Non-EntityPunctuation   0   0  37      1.0000           15\n",
       "NonEntityWords          0   0  37      0.9351           77"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wiser.eval import score_tagging_rules\n",
    "score_tagging_rules(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good rule of thumb is to write tagging rules whose accuracy is above 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking Rules\n",
    "Linking rules are simple functions that vote on whether two or more adjacent tokens belong should belong to the same entity. To get started with linking rules, you can import the ``LinkingRule`` class from ``wiser.lf``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiser.rules import LinkingRule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Linking Rules\n",
    "Tagging rules do not always correctly vote on *all* the tokens in multi-span entities. For instance, the **MovieYear** tagging rule only tags the last token in a movie span. Given the text span ``The Great Gatsby (2013)``, it only identifies the token ``Gatsby`` as **I-PERF**.\n",
    "\n",
    "Our job is to ensure that the entire class spans are tagged correctly. Therefore, we can start by writing a linking rule to indicate that consecutively capitalized words should share the same tag. Therefore, voting that ``The`` and ``Great`` share the same tag as ``Gatsby`` would tag the entire movie name as **I-PERF**, rather than the last token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveCapitals(LinkingRule):\n",
    "    \n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [t.text for t in instance['tokens']]\n",
    "        links = [0] * len(tokens)\n",
    "        \n",
    "        for i in range(1, len(tokens)):\n",
    "            if tokens[i].istitle() and tokens[i-1].istitle():\n",
    "                links[i] = 1 # token at index \"i\" shares tag with token at index \"i-1\"\n",
    "        return links\n",
    "\n",
    "lr = ConsecutiveCapitals()\n",
    "lr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our data we have also observed several movie and award names that have hyphens, colons or semicolons (e.g. ``Avengers: Endgame``). We can write a linking rule to indicate that these linking punctuation characters, along with their preceding and succeeding tokens, should all be a part of the same entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkers = {':', ';', '-'}\n",
    "\n",
    "class PunctuationLinkers(LinkingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [t.text for t in instance['tokens']]\n",
    "        links = [0] * len(tokens)\n",
    "        \n",
    "        for i in range(1, len(tokens)-1):\n",
    "            if tokens[i] in linkers:\n",
    "                \n",
    "                # The linking punctuation character and it's succeeding character\n",
    "                # share the same tag as the preceding one at index \"i-1\"\n",
    "                links[i] = 1\n",
    "                links[i+1] = 1\n",
    "        return links\n",
    "\n",
    "lr = PunctuationLinkers()\n",
    "lr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can write a rule to indicate that contractions share the same tag with the token preceding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_suffixes = {'\\'s', '\\'nt', '\\'ve', '\\'', '\\'d'}\n",
    "\n",
    "class Contractions(LinkingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [t.text for t in instance['tokens']]\n",
    "        links = [0] * len(tokens)\n",
    "        \n",
    "        for i in range(1, len(tokens)):\n",
    "            if tokens[i] in contraction_suffixes:\n",
    "                links[i] = 1\n",
    "        return links\n",
    "\n",
    "lr = Contractions()\n",
    "lr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also link noun phrases that using a list of common prepositions in award and movie names. These prepositions are part of award and movie names, and are usually lowercase and adjacent to or other prepositions or capitalized words. For example, ``Golden Globe for Best Actor`` or ``Guardians of the Galaxy``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_prepositions = {'a', 'the', 'at', 'with', 'of', 'by', '&', 'with'}\n",
    "\n",
    "class CommonPrepositions(LinkingRule):\n",
    "\n",
    "    def apply_instance(self, instance):\n",
    "        tokens = [t.text for t in instance['tokens']]\n",
    "        links = [0] * len(tokens)\n",
    "        \n",
    "        for i in range(1, len(tokens)-1):\n",
    "            if tokens[i] in common_prepositions:\n",
    "                if tokens[i-1].istitle() or tokens[i-1] in common_prepositions:\n",
    "                    if tokens[i+1].istitle() or tokens[i+1] in common_prepositions:\n",
    "                        links[i] = 1\n",
    "                        links[i+1] = 1\n",
    "        return links\n",
    "\n",
    "lr = CommonPrepositions()\n",
    "lr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking Rule Helpers\n",
    "\n",
    "Similar to tagging rules, we have linking rule helpers available at ``wiser.rules``. For the next linking rule, we will use the ``ElmoLinkingRule``, a rule that vectorizes tokens using [Elmo](https://allennlp.org/elmo) and links those with a cosine similaritiy larger than a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiser.rules import ElmoLinkingRule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Links tokens whose cosine similarity is larger than 0.8\n",
    "lr = ElmoLinkingRule(0.8)\n",
    "lr.apply(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Linking Rules\n",
    "\n",
    "Similar to tagging rules, we can evaluate the accuracy of our linking rules using the ``score_linking_functions`` method.\n",
    "\n",
    "* Entity Links represents the number of correct links generated for positive classes.\n",
    "* Non-Entity Links represents the number of correct links generated for negative classes (e.g., **O** tags).\n",
    "* Incorrect links represent the total number of incorrectly generated links.\n",
    "* Accuracy represents the fraction of issued links that identified correct links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity Links</th>\n",
       "      <th>Non-Entity Links</th>\n",
       "      <th>Incorrect Links</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CommonPrepositions</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ConsecutiveCapitals</th>\n",
       "      <td>57</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contractions</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ElmoLinkingRule</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PunctuationLinkers</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Entity Links  Non-Entity Links  Incorrect Links  Accuracy\n",
       "CommonPrepositions              5                 4                0     1.000\n",
       "ConsecutiveCapitals            57                18                0     1.000\n",
       "Contractions                    1                 0                0     1.000\n",
       "ElmoLinkingRule                 0                 2                0     1.000\n",
       "PunctuationLinkers              2                 5                1     0.875"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wiser.eval import score_linking_rules\n",
    "score_linking_rules(dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, a good rule of thumb is to have all linking rules with an accuracy above 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Progress\n",
    "We can use pickle to store the data with the tagging and linking rules applied to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n"
     ]
    }
   ],
   "source": [
    "import pickle,os\n",
    "print(os.getcwd())\n",
    "with open('/Users/yueyang/Documents/bats/wiser/output/tmp/train_data.p', 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "with open('/Users/yueyang/Documents/bats/wiser/output/tmp/dev_data.p', 'wb') as f:\n",
    "    pickle.dump(dev_data, f)\n",
    "\n",
    "with open('/Users/yueyang/Documents/bats/wiser/output/tmp/test_data.p', 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have completed part 1! Now you can move on to part 2."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
